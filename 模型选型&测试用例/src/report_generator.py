"""
æŠ¥å‘Šç”Ÿæˆå™¨ï¼šç”Ÿæˆæµ‹è¯•ç»“æœåˆ†æå’Œå¯¹æ¯”æŠ¥å‘Š
"""
import json
import os
from typing import Dict, List
from collections import defaultdict
from tabulate import tabulate
try:
    import pandas as pd
    HAS_PANDAS = True
except ImportError:
    HAS_PANDAS = False


class ReportGenerator:
    """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, results_path: str):
        self.results = self._load_results(results_path)
        self.models = set()
        self.test_cases = {}
        
    def _load_results(self, path: str) -> Dict:
        """åŠ è½½æµ‹è¯•ç»“æœ"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"âŒ åŠ è½½æµ‹è¯•ç»“æœå¤±è´¥: {e}")
            return {}
    
    def generate_summary_report(self) -> str:
        """ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š"""
        results = self.results.get("results", [])
        if not results:
            return "æ— æµ‹è¯•ç»“æœ"
        
        # æŒ‰æ¨¡å‹åˆ†ç»„
        model_stats = defaultdict(lambda: {
            "total_score": [],
            "latency": [],
            "success_count": 0,
            "error_count": 0,
            "scores_by_dimension": defaultdict(list)
        })
        
        for result in results:
            model = result.get("model", "Unknown")
            self.models.add(model)
            
            if "error" in result:
                model_stats[model]["error_count"] += 1
                continue
            
            model_stats[model]["success_count"] += 1
            model_stats[model]["total_score"].append(result.get("total_score", 0))
            model_stats[model]["latency"].append(result.get("latency", 0))
            if "ttfb" in result:
                if "ttfb" not in model_stats[model]:
                    model_stats[model]["ttfb"] = []
                model_stats[model]["ttfb"].append(result.get("ttfb", 0))
            
            # æ”¶é›†å„ç»´åº¦åˆ†æ•°
            scores = result.get("scores", {})
            for dimension, sub_scores in scores.items():
                if isinstance(sub_scores, dict):
                    for key, value in sub_scores.items():
                        if isinstance(value, (int, float)):
                            model_stats[model]["scores_by_dimension"][f"{dimension}_{key}"].append(value)
        
        # ç”ŸæˆæŠ¥å‘Š
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("å„¿ç«¥è‹±è¯­å£è¯­è€å¸ˆ Agent - æ¨¡å‹æµ‹è¯•æŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"æµ‹è¯•æ—¶é—´: {self.results.get('timestamp', 'Unknown')}")
        report_lines.append(f"æµ‹è¯•ç”¨ä¾‹æ•°: {self.results.get('test_config', {}).get('total_cases', 0)}")
        report_lines.append("")
        
        # æ€»ä½“ç»Ÿè®¡è¡¨
        table_data = []
        headers = ["æ¨¡å‹", "æˆåŠŸç‡", "å¹³å‡å¾—åˆ†", "å¹³å‡å»¶è¿Ÿ(s)", "é¦–tokenå»¶è¿Ÿ(s)", "æˆåŠŸæ•°", "å¤±è´¥æ•°"]
        
        for model in sorted(self.models):
            stats = model_stats[model]
            total_tests = stats["success_count"] + stats["error_count"]
            success_rate = (stats["success_count"] / total_tests * 100) if total_tests > 0 else 0
            avg_score = sum(stats["total_score"]) / len(stats["total_score"]) if stats["total_score"] else 0
            avg_latency = sum(stats["latency"]) / len(stats["latency"]) if stats["latency"] else 0
            avg_ttfb = sum(stats.get("ttfb", [])) / len(stats.get("ttfb", [1])) if stats.get("ttfb") else avg_latency
            
            table_data.append([
                model,
                f"{success_rate:.1f}%",
                f"{avg_score:.2f}",
                f"{avg_latency:.2f}",
                f"{avg_ttfb:.2f}",
                stats["success_count"],
                stats["error_count"]
            ])
        
        report_lines.append("æ€»ä½“ç»Ÿè®¡:")
        report_lines.append(tabulate(table_data, headers=headers, tablefmt="grid"))
        report_lines.append("")
        
        # å„ç»´åº¦è¯¦ç»†è¯„åˆ†
        report_lines.append("=" * 80)
        report_lines.append("å„ç»´åº¦è¯¦ç»†è¯„åˆ†")
        report_lines.append("=" * 80)
        
        dimension_map = {
            "language_ability": "è¯­è¨€èƒ½åŠ›",
            "teaching_adaptability": "æ•™å­¦é€‚é…æ€§",
            "response_performance": "å“åº”æ€§èƒ½",
            "safety_compliance": "å®‰å…¨åˆè§„",
            "cost_efficiency": "æˆæœ¬æ•ˆç›Š"
        }
        
        for dim_key, dim_name in dimension_map.items():
            report_lines.append(f"\nã€{dim_name}ã€‘")
            dim_table_data = []
            dim_headers = ["æ¨¡å‹"]
            
            # æ”¶é›†è¯¥ç»´åº¦çš„æ‰€æœ‰å­æŒ‡æ ‡
            sub_metrics = set()
            for model in self.models:
                stats = model_stats[model]
                for key in stats["scores_by_dimension"].keys():
                    if key.startswith(dim_key + "_"):
                        metric_name = key.replace(dim_key + "_", "")
                        # é‡å‘½åæ˜¾ç¤º
                        if metric_name == "latency_combined":
                            metric_name = "ç»¼åˆå»¶è¿Ÿ"
                        elif metric_name == "ttfb":
                            metric_name = "é¦–tokenå»¶è¿Ÿ"
                        elif metric_name == "latency":
                            metric_name = "æ€»å»¶è¿Ÿ"
                        sub_metrics.add((key.replace(dim_key + "_", ""), metric_name))
            
            # æŒ‰æ˜¾ç¤ºåç§°æ’åº
            sorted_metrics = sorted(sub_metrics, key=lambda x: x[1])
            dim_headers.extend([name for _, name in sorted_metrics])
            dim_headers.append("å¹³å‡åˆ†")
            
            metric_keys = [key for key, _ in sorted_metrics]
            
            for model in sorted(self.models):
                stats = model_stats[model]
                row = [model]
                dim_scores = []
                
                for metric_key, metric_name in sorted_metrics:
                    key = f"{dim_key}_{metric_key}"
                    scores = stats["scores_by_dimension"].get(key, [])
                    if scores:
                        avg = sum(scores) / len(scores)
                        row.append(f"{avg:.2f}")
                        dim_scores.append(avg)
                    else:
                        row.append("-")
                
                if dim_scores:
                    row.append(f"{sum(dim_scores) / len(dim_scores):.2f}")
                else:
                    row.append("-")
                
                dim_table_data.append(row)
            
            report_lines.append(tabulate(dim_table_data, headers=dim_headers, tablefmt="grid"))
        
        # æ¨èæ¨¡å‹
        report_lines.append("")
        report_lines.append("=" * 80)
        report_lines.append("æ¨¡å‹æ¨è")
        report_lines.append("=" * 80)
        
        # æŒ‰æ€»åˆ†æ’åº
        model_rankings = []
        for model in self.models:
            stats = model_stats[model]
            if stats["total_score"]:
                avg_score = sum(stats["total_score"]) / len(stats["total_score"])
                avg_latency = sum(stats["latency"]) / len(stats["latency"]) if stats["latency"] else 0
                avg_ttfb = sum(stats.get("ttfb", [])) / len(stats.get("ttfb", [1])) if stats.get("ttfb") else avg_latency
                model_rankings.append((model, avg_score, avg_latency, avg_ttfb))
        
        model_rankings.sort(key=lambda x: x[1], reverse=True)
        
        for i, (model, score, latency, ttfb) in enumerate(model_rankings, 1):
            report_lines.append(f"{i}. {model}")
            report_lines.append(f"   ç»¼åˆå¾—åˆ†: {score:.2f}/10")
            report_lines.append(f"   å¹³å‡å»¶è¿Ÿ: {latency:.2f}s")
            report_lines.append(f"   é¦–tokenå»¶è¿Ÿ: {ttfb:.2f}s")
            report_lines.append("")
        
        return "\n".join(report_lines)
    
    def generate_detailed_report(self) -> str:
        """ç”Ÿæˆè¯¦ç»†æŠ¥å‘Šï¼ˆåŒ…å«æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„ç»“æœï¼‰"""
        results = self.results.get("results", [])
        if not results:
            return "æ— æµ‹è¯•ç»“æœ"
        
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("è¯¦ç»†æµ‹è¯•ç»“æœ")
        report_lines.append("=" * 80)
        report_lines.append("")
        
        # æŒ‰æµ‹è¯•ç”¨ä¾‹åˆ†ç»„
        case_results = defaultdict(list)
        for result in results:
            case_id = result.get("test_case_id", "Unknown")
            case_results[case_id].append(result)
        
        for case_id in sorted(case_results.keys()):
            case_result_list = case_results[case_id]
            first_result = case_result_list[0]
            
            report_lines.append(f"æµ‹è¯•ç”¨ä¾‹: {case_id}")
            report_lines.append(f"  ç±»åˆ«: {first_result.get('test_case_category', 'Unknown')}")
            report_lines.append(f"  å¹´é¾„: {first_result.get('test_case_age_level', 'Unknown')}å²")
            report_lines.append("")
            
            for result in case_result_list:
                model = result.get("model", "Unknown")
                report_lines.append(f"  ã€{model}ã€‘")
                
                if "error" in result:
                    report_lines.append(f"    âŒ é”™è¯¯: {result.get('error')}")
                else:
                    score = result.get("total_score", 0)
                    latency = result.get("latency", 0)
                    content = result.get("content", "")[:100]  # æˆªå–å‰100å­—ç¬¦
                    
                    report_lines.append(f"    å¾—åˆ†: {score:.2f}/10")
                    report_lines.append(f"    å»¶è¿Ÿ: {latency:.2f}s")
                    report_lines.append(f"    å›å¤: {content}...")
                
                report_lines.append("")
            
            report_lines.append("-" * 80)
            report_lines.append("")
        
        return "\n".join(report_lines)
    
    def save_reports(self, output_dir: str = "results"):
        """ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶ï¼ˆä»…ä¿å­˜Excelæ ¼å¼ï¼Œç»¼åˆæŠ¥å‘Šç”±å…¶ä»–æ–¹å¼ç”Ÿæˆï¼‰"""
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¸å†ç”Ÿæˆæ±‡æ€»æŠ¥å‘Šå’Œè¯¦ç»†æŠ¥å‘Šï¼Œåªä¿ç•™ç»¼åˆæŠ¥å‘Š
        # ä¿å­˜Excelæ ¼å¼ï¼ˆå¯é€‰ï¼‰
        self._save_excel_report(output_dir)
    
    def _save_excel_report(self, output_dir: str):
        """ä¿å­˜Excelæ ¼å¼æŠ¥å‘Š"""
        if not HAS_PANDAS:
            print("âš ï¸  pandasæœªå®‰è£…ï¼Œè·³è¿‡ExcelæŠ¥å‘Šç”Ÿæˆï¼ˆå¯é€‰åŠŸèƒ½ï¼‰")
            return
            
        results = self.results.get("results", [])
        if not results:
            return
        
        # å‡†å¤‡æ•°æ®
        data = []
        for result in results:
            row = {
                "æ¨¡å‹": result.get("model", ""),
                "æµ‹è¯•ç”¨ä¾‹ID": result.get("test_case_id", ""),
                "ç±»åˆ«": result.get("test_case_category", ""),
                "å¹´é¾„": result.get("test_case_age_level", ""),
                "æ€»åˆ†": result.get("total_score", 0),
                "å»¶è¿Ÿ(s)": result.get("latency", 0),
            }
            
            # æ·»åŠ å„ç»´åº¦åˆ†æ•°
            scores = result.get("scores", {})
            for dimension, sub_scores in scores.items():
                if isinstance(sub_scores, dict):
                    for key, value in sub_scores.items():
                        row[f"{dimension}_{key}"] = value if isinstance(value, (int, float)) else ""
            
            row["å›å¤å†…å®¹"] = result.get("content", "")[:200]  # æˆªå–å‰200å­—ç¬¦
            
            if "error" in result:
                row["é”™è¯¯"] = result.get("error", "")
            
            data.append(row)
        
        df = pd.DataFrame(data)
        excel_path = os.path.join(output_dir, "test_results.xlsx")
        df.to_excel(excel_path, index=False, engine='openpyxl')
        print(f"ğŸ“ˆ ExcelæŠ¥å‘Šå·²ä¿å­˜: {excel_path}")

